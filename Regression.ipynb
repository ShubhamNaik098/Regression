{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Theoratical Questions**"
      ],
      "metadata": {
        "id": "qJQlt_u7NKlw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1.What is Simple Linear Regression**"
      ],
      "metadata": {
        "id": "2if8NpCnNYeI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans.Simple Linear Regression is a statistical method used to model the relationship between two variables:\n",
        "\n",
        "    One independent variable (X)\n",
        "\n",
        "    One dependent variable (Y)"
      ],
      "metadata": {
        "id": "pzTqoZHZNdL7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2.What are the key assumptions of Simple Linear Regression**"
      ],
      "metadata": {
        "id": "_Jh034R3NxaY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans.Key Assumptions of Simple Linear Regression\n",
        "\n",
        "1.Linearity\n",
        "\n",
        "    The relationship between the independent variable (X) and the dependent variable (Y) is linear.\n",
        "\n",
        "    This means a straight line can approximate the relationship.\n",
        "\n",
        "2.Independence of Errors\n",
        "\n",
        "    Observations are independent of each other.\n",
        "\n",
        "    No correlation should exist between residuals.\n",
        "\n",
        "3.Homoscedasticity (Constant Variance of Errors)\n",
        "\n",
        "    The variance of residuals (errors) is constant across all levels of X.\n",
        "\n",
        "    If this is violated, the model may underperform or give misleading confidence intervals.\n",
        "\n",
        "4.Normality of Residuals\n",
        "\n",
        "    The residuals (differences between actual and predicted values) should be normally distributed."
      ],
      "metadata": {
        "id": "i86e6wA6N3AA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3.What does the coefficient m represent in the equation Y=mX+c?**"
      ],
      "metadata": {
        "id": "2MEoJRkeOtne"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans.The coefficient m represents the slope of the line.\n",
        "\n",
        "Meaning of m:\n",
        "\n",
        "    It tells you how much Y changes for each unit increase in X.\n",
        "\n",
        "    In other words, it’s the rate of change of the dependent variable Y with respect to the independent variable X."
      ],
      "metadata": {
        "id": "ipz_ZdqRO19G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4. What does the intercept c represent in the equation Y=mX+c**"
      ],
      "metadata": {
        "id": "AICYSTzVPDG3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans.The coefficient c represents the intercept (also called the Y-intercept).\n",
        "\n",
        "Meaning of c:\n",
        "\n",
        "    It is the value of Y when X = 0.\n",
        "\n",
        "    Graphically, it is the point where the line crosses the Y-axis."
      ],
      "metadata": {
        "id": "cfFUsZjsPG9v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5.How do we calculate the slope m in Simple Linear Regression**"
      ],
      "metadata": {
        "id": "lAumip_3PRmf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans.In simple linear regression, the goal is to fit a line:\n",
        "\n",
        "    Y=mX+c\n",
        "\n",
        "Where:\n",
        "\n",
        "    m is the slope,\n",
        "\n",
        "    c is the intercept.\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "uJU_a7GUPWNm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q6.What is the purpose of the least squares method in Simple Linear Regression.**"
      ],
      "metadata": {
        "id": "gH8oXwgkQmQb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans.The least squares method is used to find the best-fitting line through a set of data points in simple linear regression."
      ],
      "metadata": {
        "id": "RRPPryN-QsIT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q7.How is the coefficient of determination (R²) interpreted in Simple Linear Regression**"
      ],
      "metadata": {
        "id": "bxJvPzs_Q3nR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans.The coefficient of determination, denoted as R², measures how well the regression line fits the data. It tells you the proportion of the variance in the dependent variable (Y) that is explained by the independent variable (X)."
      ],
      "metadata": {
        "id": "DeRbSfW6Q7a4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q8.What is Multiple Linear Regression?**"
      ],
      "metadata": {
        "id": "3TpDgY7eR_c_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans.Multiple Linear Regression is an extension of Simple Linear Regression used when there are two or more independent variables (predictors) to predict a single dependent variable."
      ],
      "metadata": {
        "id": "ENKgnAgMSFON"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q9. What is the main difference between Simple and Multiple Linear Regression?**"
      ],
      "metadata": {
        "id": "dU1ydd4MSkzA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans.Simple Linear Regression models the relationship between one independent variable and one dependent variable.\n",
        "\n",
        "Multiple Linear Regression models the relationship between two or more independent variables and one dependent variable."
      ],
      "metadata": {
        "id": "cnpWeP1VSrNg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q10.What are the key assumptions of Multiple Linear Regression?**"
      ],
      "metadata": {
        "id": "aAsyRkTCTUEZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans.Key Assumptions of Multiple Linear Regression\n",
        "\n",
        "1.Linearity\n",
        "\n",
        "    The relationship between the dependent variable and each independent variable is linear.\n",
        "\n",
        "    This can be visually assessed with scatter plots or residual plots.\n",
        "\n",
        "2.Independence of Errors\n",
        "\n",
        "    Observations should be independent of each other.\n",
        "\n",
        "    The residuals (errors) should also be independent.\n",
        "\n",
        "3.Homoscedasticity (Constant Variance of Errors)\n",
        "\n",
        "    The variance of residuals should be constant across all levels of independent variables.\n",
        "\n",
        "    No pattern in residuals vs. fitted values plot.\n",
        "\n",
        "4.Normality of Residuals\n",
        "\n",
        "    The residuals should be approximately normally distributed.\n",
        "\n",
        "    This assumption is critical for confidence intervals and hypothesis tests."
      ],
      "metadata": {
        "id": "Q5irDP7LTjrf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q11.What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?**"
      ],
      "metadata": {
        "id": "zyLmDqxHUDrL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans.Heteroscedasticity refers to a situation in regression analysis where the variance of the residuals (errors) is not constant across all levels of the independent variables. In simpler terms, the spread (or \"scatter\") of the residuals changes as the predicted value or independent variables change.\n",
        "\n",
        "Effects of Heteroscedasticity on Multiple Linear Regression:\n",
        "\n",
        "1.Inefficient Estimates:\n",
        "\n",
        "    The estimated coefficients (slopes) remain unbiased, but the estimates are not efficient (i.e., they do not have minimum variance).\n",
        "\n",
        "2.Invalid Standard Errors:\n",
        "\n",
        "    The standard errors of coefficients become biased, leading to unreliable hypothesis tests.\n",
        "\n",
        "3.Wrong Confidence Intervals and p-values:\n",
        "\n",
        "    Because of biased standard errors, confidence intervals may be too narrow or too wide.\n",
        "\n",
        "    p-values can be misleading, causing incorrect conclusions about significance.\n",
        "\n",
        "4.Model Misinterpretation:\n",
        "\n",
        "    The regression results may appear significant when they are not, or vice versa."
      ],
      "metadata": {
        "id": "PNwkvfvNUQUl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q12.How can you improve a Multiple Linear Regression model with high multicollinearity?**"
      ],
      "metadata": {
        "id": "3VOyChvmUn7J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans.Multicollinearity occurs when two or more independent variables in a regression model are highly correlated.\n",
        "\n",
        "This causes problems like unstable coefficient estimates and inflated standard errors.\n",
        "\n",
        "\n",
        "1.Remove Highly Correlated Predictors\n",
        "\n",
        "    Identify highly correlated variables (e.g., correlation > 0.8 or VIF > 5 or 10).\n",
        "\n",
        "    Remove or combine redundant variables.\n",
        "\n",
        "2.Feature Selection Techniques\n",
        "\n",
        "    Use methods like stepwise regression, Lasso regression, or Elastic Net which perform variable selection and shrinkage.\n",
        "\n",
        "3.Principal Component Analysis (PCA)\n",
        "  \n",
        "    Transform correlated variables into a smaller set of uncorrelated components.\n",
        "\n",
        "    Use these components as predictors instead of original variables.\n",
        "\n",
        "4.Regularization Methods\n",
        "    \n",
        "    Use Ridge Regression or Lasso Regression that add penalties to reduce coefficient magnitudes, helping with multicollinearity."
      ],
      "metadata": {
        "id": "4ZESsQlpUtEO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q13.What are some common techniques for transforming categorical variables for use in regression models?**"
      ],
      "metadata": {
        "id": "EzB9cAWxVJWu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans."
      ],
      "metadata": {
        "id": "tpQQN4qyVOv2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q14.What is the role of interaction terms in Multiple Linear Regression?**"
      ],
      "metadata": {
        "id": "6qUoX1MxVP5_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans.Interaction terms represent the combined effect of two or more independent variables on the dependent variable that is not simply additive.\n",
        "\n",
        "They capture situations where the effect of one predictor depends on the value of another predictor.\n",
        "\n",
        "Why Use Interaction Terms?\n",
        "\n",
        "    To model non-additive relationships between variables.\n",
        "\n",
        "    To explore whether two variables together influence the outcome differently than expected from their individual effects.\n",
        "\n",
        "    To improve model accuracy and interpretability when interactions exist.\n",
        "\n"
      ],
      "metadata": {
        "id": "n3KW9GL-VVo6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q15.How can the interpretation of intercept differ between Simple and Multiple Linear Regression?**"
      ],
      "metadata": {
        "id": "uxuXRRXGVrqe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans.Simple LR intercept = baseline value of Y when the single predictor is zero.\n",
        "\n",
        "Multiple LR intercept = baseline value of Y when all predictors are zero simultaneously."
      ],
      "metadata": {
        "id": "DrJ47xMoVwVi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q16.What is the significance of the slope in regression analysis, and how does it affect predictions?**"
      ],
      "metadata": {
        "id": "3-BGbi4iV9eX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans.The slope (often denoted as b1 in simple linear regression) represents the rate of change in the dependent variable Y for a one-unit increase in the independent variable X, holding other variables constant (in multiple regression)."
      ],
      "metadata": {
        "id": "wj0E76ZrWCS-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q17.How does the intercept in a regression model provide context for the relationship between variables?**"
      ],
      "metadata": {
        "id": "GRR5J1qbWfUe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans.The intercept (b0) is the point where the regression line crosses the Y-axis.\n",
        "\n",
        "It represents the predicted value of the dependent variable Y when all independent variables X are zero.\n",
        "\n",
        "How Does It Provide Context?\n",
        "\n",
        "1.Baseline Value:\n",
        "\n",
        "    The intercept gives a baseline or starting point for Y before considering the effect of predictors.\n",
        "\n",
        "2.Reference Point for Predictions:\n",
        "\n",
        "    It anchors the regression line, allowing you to understand how much Y would be if predictors were at their zero level.\n",
        "\n",
        "3.Interpretation Depends on Meaningfulness of X=0:\n",
        "\n",
        "    If zero is a meaningful or realistic value of X, the intercept is interpretable as a real-world expected outcome.\n",
        "\n",
        "    If zero is outside the observed data range or unrealistic (e.g., zero years of experience might be meaningful; zero temperature in Kelvin is absolute zero and meaningful; zero age might not be meaningful in some contexts), the intercept serves more as a mathematical artifact.\n",
        "\n",
        "4.Context in Multiple Regression:\n",
        "\n",
        "    When there are multiple predictors, the intercept is the expected value of Y when all predictors are zero simultaneously, which might be rare or unrealistic, but still provides a baseline for the model."
      ],
      "metadata": {
        "id": "kqYJoybOWkxG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q18.What are the limitations of using R² as a sole measure of model performance?**"
      ],
      "metadata": {
        "id": "Ek_rd7j4XPks"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans.Limitations of Using R² as the Sole Measure of Model Performance\n",
        "\n",
        "1.Does Not Indicate Causation or Model Validity\n",
        "\n",
        "    A high R² means the model explains a large proportion of variance in the dependent variable, but it does not imply causation.\n",
        "\n",
        "    The model could be overfitting or miss important variables.\n",
        "\n",
        "2.Insensitive to Overfitting\n",
        "\n",
        "    Adding more predictors always increases or at least doesn’t decrease R², even if those variables are irrelevant.\n",
        "\n",
        "    This can give a misleading impression of a better model."
      ],
      "metadata": {
        "id": "OpshpjNCXWo3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q19. How would you interpret a large standard error for a regression coefficient?**"
      ],
      "metadata": {
        "id": "sYZxiyVdXpl7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans.Standard Error of a Coefficient:\n",
        "\n",
        "    The standard error (SE) of a regression coefficient measures the variability or uncertainty in the estimated coefficient.\n",
        "\n",
        "    It tells us how much the estimated coefficient would vary if we repeated the study with different samples."
      ],
      "metadata": {
        "id": "fjkmpywxXutS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q20.How can heteroscedasticity be identified in residual plots, and why is it important to address it?**"
      ],
      "metadata": {
        "id": "fLm9xX6yYFBB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans.Heteroscedasticity:\n",
        "\n",
        "    Heteroscedasticity occurs when the variance of the residuals (errors) is not constant across all levels of the independent variable(s).\n",
        "\n",
        "    It violates one of the key assumptions of linear regression — constant variance of errors (homoscedasticity).\n",
        "\n",
        "Why is Addressing Heteroscedasticity Important?\n",
        "\n",
        "1.Violates Regression Assumptions:\n",
        "\n",
        "    Linear regression assumes constant variance of errors for valid inference.\n",
        "\n",
        "2.Impacts Standard Errors and Tests:\n",
        "\n",
        "    Heteroscedasticity leads to biased standard errors → incorrect confidence intervals and hypothesis tests (e.g., t-tests, F-tests).\n",
        "\n",
        "3.Affects Efficiency of Estimates:\n",
        "\n",
        "    Ordinary Least Squares (OLS) estimators remain unbiased but are no longer efficient (not minimum variance).\n",
        "\n",
        "4.Misleading Model Interpretation:\n",
        "\n",
        "    Significance levels and p-values can be unreliable, leading to wrong conclusions."
      ],
      "metadata": {
        "id": "gfh48bwgYJuS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q21.Misleading Model Interpretation:\n",
        "Significance levels and p-values can be unreliable, leading to wrong conclusions.**"
      ],
      "metadata": {
        "id": "o2oHsSQkYl9G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans."
      ],
      "metadata": {
        "id": "trM4Z_e9Yrf2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q22. Why is it important to scale variables in Multiple Linear Regression?**"
      ],
      "metadata": {
        "id": "hSinPjaFYsm4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans.1.It Important to Scale Variables in Multiple Linear Regression in te following:\n",
        "\n",
        "1.Ensures Comparable Scales\n",
        "\n",
        "    Variables in different units (e.g., age in years, income in thousands, height in cm) can have vastly different scales.\n",
        "\n",
        "    Scaling puts all variables on a comparable scale, preventing variables with larger magnitudes from dominating the model estimation.\n",
        "\n",
        "2.Improves Numerical Stability\n",
        "\n",
        "    Large differences in scales can cause numerical instability during matrix calculations in regression algorithms.\n",
        "\n",
        "    Scaling helps algorithms converge faster and more reliably.\n",
        "\n",
        "3.Helps Interpret Coefficients (When Standardized)\n",
        "\n",
        "    When variables are scaled (e.g., standardized to mean=0 and std=1), regression coefficients represent the effect of a one standard deviation change in predictors on the outcome.\n",
        "\n",
        "    This makes coefficients directly comparable to understand variable importance."
      ],
      "metadata": {
        "id": "rrSdRJ0RYxhS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q23.What is polynomial regression?**"
      ],
      "metadata": {
        "id": "fSfxJudKZMIw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans.Polynomial Regression is an extension of simple linear regression where the relationship between the independent variable X and the dependent variable Y is modeled as an nth-degree polynomial rather than a straight line."
      ],
      "metadata": {
        "id": "zFnLqhfAZZB_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q24. How does polynomial regression differ from linear regression?**"
      ],
      "metadata": {
        "id": "-eZjCYS7Zm97"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans.Linear regression fits a straight line; polynomial regression fits a curve.\n",
        "\n",
        "Polynomial regression is essentially a linear regression on transformed features (powers of X).\n",
        "\n",
        "Polynomial regression can model more complex relationships but requires care to avoid overfitting."
      ],
      "metadata": {
        "id": "M1lZvTo0Zrst"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q25. When is polynomial regression used?**"
      ],
      "metadata": {
        "id": "vov9Ou3UZ4NB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans.When to Use Polynomial Regression?\n",
        "\n",
        "    When data shows a curved trend that linear regression cannot fit well.\n",
        "\n",
        "    To better approximate complex patterns in the data."
      ],
      "metadata": {
        "id": "fyhPrDt-aAGo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**26.What is the general equation for polynomial regression?**"
      ],
      "metadata": {
        "id": "3D7een4KaC2J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans."
      ],
      "metadata": {
        "id": "vEkwLpSAaI5L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q27.Can polynomial regression be applied to multiple variables?**"
      ],
      "metadata": {
        "id": "w0zZb9VLaNpD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans.This is known as Multivariate Polynomial Regression or Polynomial Regression with multiple predictors."
      ],
      "metadata": {
        "id": "OD5AGy-3aS40"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q28. What are the limitations of polynomial regression?**"
      ],
      "metadata": {
        "id": "caPWAqAvahge"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans.Limitations of Polynomial Regression:\n",
        "\n",
        "1.Overfitting\n",
        "    \n",
        "    High-degree polynomials can fit the training data too well, including noise, leading to poor generalization to new data.\n",
        "\n",
        "    The model becomes overly complex and sensitive to small fluctuations.\n",
        "\n",
        "2.Extrapolation Problems\n",
        "\n",
        "    Predictions outside the range of the training data (extrapolation) can become extremely unreliable, especially for higher degrees.\n",
        "\n",
        "    Polynomial functions tend to oscillate wildly at the edges.\n",
        "\n",
        "3. Interpretability\n",
        "    \n",
        "    As the degree increases, the model becomes harder to interpret.\n",
        "\n",
        "4.Computational Complexity\n",
        "\n",
        "    The number of features increases exponentially with the degree and number of input variables.\n",
        "\n",
        "    This increases training time and risk of multicollinearity (especially in multivariate polynomial regression)."
      ],
      "metadata": {
        "id": "PIRk8ifQalxC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q29.What methods can be used to evaluate model fit when selecting the degree of a polynomial?**"
      ],
      "metadata": {
        "id": "ybHGVIHPbDEx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans."
      ],
      "metadata": {
        "id": "h7ApPVcfbGjg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q30.Why is visualization important in polynomial regression?**"
      ],
      "metadata": {
        "id": "EMPOaRN4bHYn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans.Importance of Visualization in Polynomial Regression:\n",
        "\n",
        "Visualization plays a crucial role in understanding, evaluating, and communicating the results of a polynomial regression model. Here's why it's important:\n",
        "\n",
        "1.Understanding the Model Fit\n",
        "\n",
        "    Polynomial regression can produce curved lines—visualization helps you see the shape of that fit.\n",
        "\n",
        "    You can assess whether the curve actually follows the data trend or if it’s over/underfitting.\n",
        "\n",
        "2.Detecting Overfitting\n",
        "\n",
        "    Visualization helps identify overfitting, where the curve tightly wraps around data points unnecessarily.\n",
        "\n",
        "    Overfit models often show sharp bends or oscillations.\n",
        "\n",
        "3.Assessing Residuals\n",
        "\n",
        "    Plotting residuals (differences between predicted and actual values) helps spot:\n",
        "\n",
        "        Non-random patterns → bad fit\n",
        "\n",
        "        Heteroscedasticity → unequal variance\n",
        "\n",
        "        Outliers → extreme data points affecting the model\n",
        "\n",
        "4.Communicating Insights\n",
        "\n",
        "    Visual plots make it easier to explain model behavior to non-technical stakeholders.\n",
        "\n",
        "    They can see how the model curves, reacts to inputs, and where it may fail.\n"
      ],
      "metadata": {
        "id": "h-zS7yfpbL5_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q31.How is polynomial regression implemented in Python?**"
      ],
      "metadata": {
        "id": "66_oIqLpbycw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans."
      ],
      "metadata": {
        "id": "VCou-wSxb14j"
      }
    }
  ]
}